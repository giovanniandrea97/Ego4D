# Ego4D
Project_Ego4D_NLQ_Benchmark
This project explores the use of natural language queries for egocentric video understanding, leveraging the exten- sive Ego4D dataset and its NLQ benchmark. Our work is divided into two phases: video moment localization and the generation of textual answers based on the identified moments. In the first phase, we address the NLQ task by identifying the temporal segment of the video where a given natural language query is answerable. We model this as a regression problem using video span localizing networks: VSLBase and its enhanced version, VSLNet. Both networks learn cross-modal interactions between video and text fea- tures through an attention-like mechanism, and they are able to regress the span of the target moment. By incor- porating the Query-Guided Highlighting (QGH) module, VSLNet improves performance by focusing the search of the target moment within the highlighted region. Additionally, we approach the task as a matching problem using a Tem- poral Adjacent Network (2D-TAN), which selects the opti- mal moment from a temporal 2D map. In the second phase, we extend the project by extracting textual answers from the identified target moments using a large vision-language model, Video-LLaVA. This project demonstrates progress towards developing an augmented reality assistant capable of interpreting daily-life egocentric videos and responding to queries about past activities.
